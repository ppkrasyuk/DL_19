# -*- coding: utf-8 -*-
"""Log_reg_corrected.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-CnRUgaB7fpVotOa1jynlTDpUa4Y6sOe
"""

from google.colab import drive
import numpy as np
import cv2
from sklearn.model_selection import train_test_split
import os

drive.mount('/content/gdrive')
path = "/content/gdrive/My Drive/"

def read_files(X, Y, path, ans):
  files = os.listdir(path)
  for name in files:
    img = cv2.imread(path + '/' + name, 0)
    if img.shape != 0:
      img = cv2.resize(img, (256, 256))
      vect = img.reshape(1, 256 ** 2)
      vect = vect / 255.
      X = vect if (X is None) else np.vstack((X, vect)) 
      Y = np.append(Y, ans)
  return X, Y

def init_model(input_size=256):
  """
  This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.
  Argument:
  dim -- size of the w vector we want (or number of parameters in this case)
  Returns:
  w -- initialized vector of shape (dim, 1)
  b -- initialized scalar (corresponds to the bias)
  """
  w = np.zeros((input_size ** 2, 1))
  b = 0
  return w, b

def sigmoid(z):
  """
  Compute the sigmoid of z
  Arguments:
  z -- A scalar or numpy array of any size.
  Return:
  s -- sigmoid(z)
  """
  return 1./(1 + np.exp(-z))

def propagate(w, b, X, Y):
  
  """
  Implement the cost function and its gradient for the propagation explained above
  Arguments:
  w -- weights, a numpy array of size (num_px * num_px * 3, 1)
  b -- bias, a scalar
  X -- data of size (num_px * num_px * 3, number of examples)
  Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)
  Return:
  cost -- negative log-likelihood cost for logistic regression
  dw -- gradient of the loss with respect to w, thus same shape as w
  db -- gradient of the loss with respect to b, thus same shape as b
  Tips:
  - Write your code step by step for the propagation. np.log(), np.dot()
  """
  # FORWARD PROPAGATION (FROM X TO COST)
  z = np.dot(w.T, X) + b
  m = z.shape[1]
  A = sigmoid(z)
  
  # calculating error function
  L = Y * np.log(A) + (1 - Y) * np.log(1 - A)
  cost = - L.sum() / m
  # BACKWARD PROPAGATION (TO FIND GRAD)
  # calculating grads
  dw = np.dot(X, (A - Y).T) / m
  db = (A - Y).sum() / m
  
  return dw, db, cost

def optimize(w, b, X, Y, num_iterations, learning_rate):
  """
  This function optimizes w and b by running a gradient descent algorithm
  Arguments:
  w -- weights, a numpy array of size (num_px * num_px * 3, 1)
  b -- bias, a scalar
  X -- data of shape (num_px * num_px * 3, number of examples)
  Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)
  num_iterations -- number of iterations of the optimization loop
  learning_rate -- learning rate of the gradient descent update rule
  print_cost -- True to print the loss every 100 steps
  Returns:
  params -- dictionary containing the weights w and bias b
  grads -- dictionary containing the gradients of the weights and bias with respect to the cost function
  costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.
  Tips:
  You basically need to write down two steps and iterate through them:
      1) Calculate the cost and the gradient for the current parameters. Use propagate().
      2) Update the parameters using gradient descent rule for w and b.
  """
  costs = []
  for i in range(num_iterations):
    # Cost and gradient calculation
    dw, db, cost = propagate(w, b, X, Y)
    # Record the costs
    costs.append(cost)
    # update rule
    w = w - learning_rate * dw
    b = b - learning_rate * db
  return w, b, costs

def threshold(A, thr):
  m = A.shape[1]
  res = np.zeros((1, m))
  for i in range(m):
    res[0, i] = 1 if (A[0, i] > thr) else 0
  return res

def predict(w, b, X, thr=0.5):
  '''
  Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)
  Arguments:
  w -- weights, a numpy array of size (num_px * num_px * 3, 1)
  b -- bias, a scalar
  X -- data of size (num_px * num_px * 3, number of examples)
  Returns:
  Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X
  '''
  #Compute vector "A" predicting the probabilities of a cat being present in the picture
  z = np.dot(w.T, X) + b
  A = sigmoid(z)
  Y_prediction = threshold(A, thr)
  return Y_prediction

def accuracy(Y, Y_prediction):
  diff = np.abs(Y - Y_prediction)
  num = Y.shape[1] - diff.sum()
  denom = Y.shape[1]
  return float(num/denom)

"""Parse data into arrays X, Y:"""

path = "/content/gdrive/My Drive/datasets"
X = None
Y = np.array([])
X, Y = read_files(X, Y, path + "/logloss_0", 0)
X, Y = read_files(X, Y, path + "/logloss_1", 1)

"""Actions:"""

"""
    Builds the logistic regression model by calling the function you've implemented previously
    Arguments:
    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)
    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)
    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)
    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)
    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters
    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()
    print_cost -- Set to true to print the cost every 100 iterations
    Returns:
    d -- dictionary containing information about the model.
    """
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
X_train = X_train.T
X_test = X_test.T
Y_train = Y_train[np.newaxis, :]
Y_test = Y_test[np.newaxis, :]
# initialize parameters 
w, b = init_model()
#Gradient descent
w, b, costs = optimize(w, b, X_train, Y_train, num_iterations=1000, learning_rate=0.003)
# Predict test/train set examples
Y_prediction = predict(w, b, X_test)
print(Y_test)
print(Y_prediction)
q = accuracy(Y_test, Y_prediction) # q = number of passed tests / number of tests
print(q)

"""Mount google disk:"""